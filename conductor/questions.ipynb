{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Engineering Interview Q&A\n",
    "\n",
    "---\n",
    "\n",
    "### Intermediate Level\n",
    "\n",
    "**What is ETL, and when would you choose ELT over ETL?**  \n",
    "ETL (Extract-Transform-Load) extracts data, transforms it, and then loads it into a destination. ELT (Extract-Load-Transform) loads data first into a storage system (like a data warehouse or data lake), and then transforms it in place.  \n",
    "Use ELT with cloud data warehouses like BigQuery, Snowflake, or Redshift where transformation performance is optimized within the platform.\n",
    "\n",
    "**Explain partitioning and bucketing. How do they help performance?**  \n",
    "Partitioning divides data into logical segments (e.g., by date) for efficient scanning. Bucketing distributes data across fixed buckets using a hash function, which improves join and aggregation performance by organizing data more evenly.\n",
    "\n",
    "**What’s the difference between a Data Lake and a Data Warehouse? When to use each?**  \n",
    "A Data Lake stores raw, unstructured, and semi-structured data and is ideal for ML, AI, and flexible schema use cases. A Data Warehouse is optimized for structured data and fast querying for business intelligence.\n",
    "\n",
    "**Describe idempotency in data pipelines. How would you ensure it?**  \n",
    "Idempotency ensures that repeated execution does not change the result beyond the initial application. Achieve it through unique identifiers, upserts, deduplication logic, and side-effect-free operations.\n",
    "\n",
    "**Batch vs. Streaming processing—what’s the difference, and when would you choose streaming?**  \n",
    "Batch processing handles large volumes of data at scheduled intervals—suitable for reports or periodic analytics. Streaming processing handles data in near real-time—ideal for dashboards, alerting, and immediate decision-making.\n",
    "\n",
    "**How would you handle schema evolution in a production pipeline?**  \n",
    "Use schema registries, versioned data formats like Avro/Parquet, and build logic to adapt to or validate schema changes gracefully.\n",
    "\n",
    "**What are some techniques to optimize Spark jobs?**  \n",
    "Use predicate pushdown, broadcast joins for small lookup tables, avoid wide transformations, cache intermediate results, and tune partitioning and memory configurations.\n",
    "\n",
    "**What is the role of metadata in a data platform?**  \n",
    "Metadata provides context (e.g., schema, lineage, quality, usage stats). It enables governance, discovery, debugging, and compliance audits.\n",
    "\n",
    "---\n",
    "\n",
    "### Senior Level\n",
    "\n",
    "**How would you design a scalable data pipeline on AWS?**  \n",
    "Ingest data to Amazon S3, process it using AWS Glue or EMR with Spark, store transformed data in Redshift or Athena, and orchestrate using Step Functions or Airflow. For real-time ingestion, use Kinesis.\n",
    "\n",
    "**How would your design differ on GCP?**  \n",
    "Use Cloud Storage for raw data, Dataflow or Dataproc for processing, and store results in BigQuery. Use Pub/Sub for real-time streaming. Orchestrate with Cloud Composer (managed Airflow).\n",
    "\n",
    "**What strategies would you use to handle data skew in distributed datasets?**  \n",
    "Salt the join keys to reduce skew, pre-aggregate data, increase partitions, and avoid expensive shuffles. Monitor stage-level metrics to identify bottlenecks.\n",
    "\n",
    "**Describe methods to ensure high availability and fault tolerance in data pipelines.**  \n",
    "Use retries with exponential backoff, checkpointing in Spark/Flink, replicate data across zones/regions, automate failure alerts, and rely on managed services with built-in fault tolerance.\n",
    "\n",
    "**What is data governance, and how would you ensure compliance in your pipelines?**  \n",
    "Data governance covers quality, security, lineage, and compliance. Implement encryption, auditing, RBAC, and automated metadata/cataloging (e.g., using AWS Glue Data Catalog or Google Data Catalog).\n",
    "\n",
    "**What would be your approach to migrating an on-prem data warehouse to the cloud?**  \n",
    "Inventory current data and dependencies. Choose the right target (e.g., BigQuery/Redshift). Use migration tools (e.g., AWS DMS), validate data post-migration, and phase the rollout. Plan for security, access control, and performance tuning.\n",
    "\n",
    "**How would you design for multi-tenant data isolation in a shared warehouse?**  \n",
    "Use row-level or column-level security, separate schemas or datasets per tenant, enforce access policies, and monitor usage for cost attribution and anomaly detection.\n",
    "\n",
    "**How do you monitor and alert on data pipeline health in production?**  \n",
    "Track SLA violations, pipeline runtimes, data volume anomalies, and schema changes. Use tools like CloudWatch, Stackdriver, or Prometheus with alerting integrations (PagerDuty, Slack, etc.).\n",
    "\n",
    "**How do you manage and test infrastructure as code (IaC) for data pipelines?**  \n",
    "Use Terraform or CloudFormation to version infrastructure. Separate environments (dev/staging/prod). Implement CI/CD pipelines for deploying IaC with validations and rollbacks.\n",
    "\n",
    "**How would you secure sensitive data in a cloud-based data pipeline?**  \n",
    "Encrypt data at rest and in transit, mask PII before storage, use KMS for key management, enforce least-privilege IAM roles, and audit access logs regularly.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
